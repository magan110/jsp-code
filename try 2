# main.py

"""
AI CHATBOT + SQL SERVER SCHEMA READER (REMOTE SQL SERVER VERSION)

Before running:
    python -m pip install transformers accelerate sentencepiece pyodbc pandas
"""

import pyodbc
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


# ---------------------------------------------------------
# 1. Load AI Model
# ---------------------------------------------------------

print("Loading AI model…")

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, dtype=torch.float32
)

conversation_history = []


# ---------------------------------------------------------
# 2. SQL Server Connection String (SAFE PLACEHOLDER)
# ---------------------------------------------------------

CONNECTION_STRING = r"""
Driver={ODBC Driver 17 for SQL Server};
Server=10.4.64.18;
Database=bwlive;
Uid=sa;
Pwd=Point@0652;
Trusted_Connection=No;
TrustServerCertificate=Yes;
"""

# IMPORTANT:
# Replace YOUR_PASSWORD_HERE with your actual password locally.
# Never upload your real password to the internet.


# ---------------------------------------------------------
# 3. Read SQL Schema
# ---------------------------------------------------------

def load_database_schema():
    print("Connecting to database…")

    try:
        conn = pyodbc.connect(CONNECTION_STRING)
    except Exception as e:
        print("\n❌ DATABASE CONNECTION FAILED ❌")
        print(e)
        print("\nMake sure your SQL Server allows remote connections.")
        exit()

    cursor = conn.cursor()
    schema = {}

    tables = cursor.tables(tableType="TABLE")

    for table in tables:
        table_name = table.table_name
        schema[table_name] = {"columns": [], "primary_keys": [], "foreign_keys": []}

        # Columns
        for col in cursor.columns(table=table_name):
            schema[table_name]["columns"].append({
                "name": col.column_name,
                "type": col.type_name,
                "size": col.column_size
            })

        # Primary Keys
        for pk in cursor.primaryKeys(table=table_name):
            schema[table_name]["primary_keys"].append(pk.column_name)

        # Foreign Keys
        for fk in cursor.foreignKeys(table=table_name):
            schema[table_name]["foreign_keys"].append({
                "fk_column": fk.fkcolumn_name,
                "pk_table": fk.pktable_name,
                "pk_column": fk.pkcolumn_name
            })

    conn.close()
    print("✅ Schema loaded!\n")
    return schema


database_schema = load_database_schema()


# ---------------------------------------------------------
# 4. AI Response Generator (with schema)
# ---------------------------------------------------------

def generate_ai_response(user_input):
    global conversation_history

    schema_text = "DATABASE SCHEMA:\n"
    for table, info in database_schema.items():
        schema_text += f"\nTABLE: {table}\n"
        schema_text += "  Columns:\n"
        for col in info["columns"]:
            schema_text += f"    - {col['name']} ({col['type']})\n"
        if info["primary_keys"]:
            schema_text += f"  Primary Keys: {', '.join(info['primary_keys'])}\n"
        if info["foreign_keys"]:
            schema_text += "  Foreign Keys:\n"
            for fk in info["foreign_keys"]:
                schema_text += f"    - {fk['fk_column']} → {fk['pk_table']}.{fk['pk_column']}\n"

    conversation_history.append({"role": "user", "content": user_input})

    prompt = (
        "<|system|>You are a SQL Server expert. Use the database schema to answer queries.\n"
        + schema_text + "\n"
    )

    for msg in conversation_history:
        if msg["role"] == "user":
            prompt += f"<|user|>{msg['content']}"
        elif msg["role"] == "assistant":
            prompt += f"<|assistant|>{msg['content']}"

    prompt += "<|assistant|>"

    inputs = tokenizer(prompt, return_tensors="pt")

    output = model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

    full = tokenizer.decode(output[0], skip_special_tokens=True)

    if "<|assistant|>" in full:
        reply = full.split("<|assistant|>")[-1].strip()
    else:
        reply = full.strip()

    conversation_history.append({"role": "assistant", "content": reply})
    return reply


# ---------------------------------------------------------
# 5. Chat Loop
# ---------------------------------------------------------

def main():
    print("\nDatabase-Aware Chatbot Ready!")
    print("Ask anything about schema, tables, relationships, or SQL queries.\n")

    while True:
        user = input("You: ")

        if user.lower() in ["bye", "quit", "exit"]:
            print("AI: Goodbye!")
            break

        print("AI:", generate_ai_response(user))


if __name__ == "__main__":
    main()
