import os, json, re, glob, random, datetime, difflib, unicodedata
import duckdb
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration

# =========================
# Fixed locations & env
# =========================
MODEL_BASE  = "models/nl2sql_tiny"     # always use this model directory
DB_DIR      = "../db_export_bwlive"
CAT         = os.path.join(DB_DIR, "catalog.json")

DEBUG               = os.environ.get("AI_DB_AGENT_DEBUG", "0") == "1"
TABLE_SCAN_LIMIT    = int(os.environ.get("AI_DB_AGENT_TABLE_LIMIT", "300"))  # fallback scan tables cap
ROW_LIMIT_PER_TABLE = int(os.environ.get("AI_DB_AGENT_ROW_LIMIT", "200"))    # rows probed per table when building row expr / sampling

# =========================
# Small-talk heuristics
# =========================
GREET_WORDS = {"hi","hello","hey","yo","namaste","hola","sup","hii","hiii","hlo"}
THANK_WORDS = {"thanks","thank you","ty","thx","shukriya","dhanyavaad"}
BYE_WORDS   = {"bye","goodbye","see ya","see you","tata"}
HELP_WORDS  = {"help","commands","examples","how to","what can you do"}
JOKE_WORDS  = {"joke","funny","make me laugh"}

STOPWORDS = {
    "what","is","are","of","for","about","the","a","an","this","that","these","those",
    "show","give","get","find","list","name","names","code","codes","number","numbers",
    "top","first","last","all","and","or","with","without","please","pls","zone",
}

# value-pattern regex (data-only)
PHONE_VAL_RE = re.compile(r"^\s*\+?\s*(?:\d[\s().-]?){7,15}\s*$")
EMAIL_VAL_RE = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")

# =========================
# Utilities
# =========================
def _norm_path(p: str) -> str:
    return p.replace("\\", "/")

def _sql_escape_like(s: str) -> str:
    return (s or "").replace("'", "''")

def _clean_str(x) -> str:
    s = str(x or "").strip()
    return unicodedata.normalize("NFKC", s)

def _wanted_field(q: str):
    L = q.lower()
    if re.search(r"\b(email|e\-?mail)\b", L): return "email"
    if re.search(r"\b(mobile|phone|contact|tel|cell)\b", L): return "phone"
    return None

def _tokens_from_question(q: str):
    m = re.search(r"[\"â€œâ€'`](.+?)[\"â€œâ€'`]", q)
    raw = re.findall(r"[A-Za-z0-9@+._/-]+", m.group(1) if m else q)
    toks = []
    for t in raw:
        tl = t.lower()
        if tl in STOPWORDS: continue
        if any(c.isdigit() for c in t) or len(t) >= 3:
            toks.append(t)
    if not toks and raw:
        toks = [max(raw, key=len)]
    return toks[:6]

def _extract_name_hint(q: str):
    m = re.search(r"[\"â€œâ€'`](.+?)[\"â€œâ€'`]", q)
    if m: return m.group(1).strip()
    t = _tokens_from_question(q)
    return " ".join(t[:3]) if t else ""

def _has_model_files(d: str) -> bool:
    pats = [
        "pytorch_model.bin", "model.safetensors",
        "pytorch_model-*.bin", "model-*.safetensors",
        "pytorch_model.bin.index.json", "model.safetensors.index.json",
    ]
    return any(glob.glob(os.path.join(d, p)) for p in pats)

def _latest_checkpoint(base: str) -> str:
    if not os.path.isdir(base):
        raise FileNotFoundError(f"Model folder not found: {base}")
    cands = []
    for name in os.listdir(base):
        full = os.path.join(base, name)
        if os.path.isdir(full) and name.startswith("checkpoint-") and _has_model_files(full):
            try: cands.append((int(name.split("-")[-1]), full))
            except: pass
    if cands:
        cands.sort(key=lambda x: x[0])
        return cands[-1][1]
    if _has_model_files(base): return base
    raise FileNotFoundError(
        f"No usable model files in {base}. Put weights in {base} or checkpoint-* subfolders "
        f"(expected pytorch_model.bin or model.safetensors)."
    )

def _find_sp_model(model_dir: str) -> str:
    for name in ("spiece.model","spm.model","sentencepiece.model","tokenizer.model"):
        p = os.path.join(model_dir, name)
        if os.path.isfile(p): return os.path.abspath(p)
    # also check the model base if we selected a checkpoint subdir
    base = os.path.dirname(model_dir)
    for name in ("spiece.model","spm.model","sentencepiece.model","tokenizer.model"):
        p = os.path.join(base, name)
        if os.path.isfile(p): return os.path.abspath(p)
    raise FileNotFoundError(
        f"SentencePiece model not found in {model_dir} or its base.\n"
        f"Expected one of: spiece.model / spm.model / sentencepiece.model / tokenizer.model"
    )

def is_probably_db_query(q: str) -> bool:
    L = q.lower()
    cues = [
        r"\b(phone|mobile|contact|tel|email|e\-?mail)\b",
        r"\b(code|customer|cust|retail|retl|party|employee|emp|sap|invoice|docu|pin|pincode)\b",
        r"\b(list|show|top|count|where|select|table|rows?)\b",
        r"[A-Za-z]{2,}\d{2,}",  # IDs like PZ6115, 4101P158
    ]
    return any(re.search(x, L) for x in cues)

def small_talk(q: str) -> str:
    L = q.strip().lower()
    words = set(re.findall(r"[a-z]+", L))
    if words & GREET_WORDS:
        return random.choice(["hey! how can i help?","hello! ask me anything about your dataâ€”or just chat ðŸ™‚","hi! need a phone/email/code or some quick info?"])
    if words & THANK_WORDS:
        return random.choice(["anytime!","youâ€™re welcome âœ¨","glad to help."])
    if words & BYE_WORDS:
        return random.choice(["bye! have a good one ðŸ‘‹","see you later!","catch you soon!"])
    if any(x in L for x in HELP_WORDS):
        return ("iâ€™m your data assistant + chat buddy. try:\n"
                "â€¢ phone number of \"rajendra shikwal\"\n"
                "â€¢ email of PZ6115\n"
                "â€¢ top 5 adtAnnGifts\n"
                "â€¢ name of all tables\n"
                "you can also just chat: â€œtell me a jokeâ€, â€œwhat can you doâ€, etc.")
    if "time" in L or "date" in L or "today" in L:
        now = datetime.datetime.now()
        return f"itâ€™s {now.strftime('%Y-%m-%d %H:%M:%S')} on your machine."
    if any(x in L for x in JOKE_WORDS):
        return random.choice([
            "why did the database admin break up with sql? too many joins. ðŸ˜…",
            "i told my boss i know sql. they said: prove it. i replied: SELECT * FROM compliments;",
        ])
    return random.choice(["got itâ€”ask me anything about your database, or say 'help' for examples.",
                          "iâ€™m here. what do you want to find?",
                          "cool. tell me who/what youâ€™re looking for and iâ€™ll fetch it."])

# =========================
# Row-only search helpers
# =========================
def _row_text_expr(cols):
    """
    Concatenate ALL cell values of a row into one VARCHAR string,
    so matching is strictly on row data (not column/table names).
    """
    if not cols:
        return "''"
    casts = [f"COALESCE(CAST(\"{c}\" AS VARCHAR), '')" for c in cols]
    return " || ' ' || ".join(casts)

def _extract_values_by_regex(df: pd.DataFrame, val_re: re.Pattern, k: int = 5):
    vals = []
    for c in df.columns:
        try:
            s = df[c].dropna().astype(str).str.strip()
        except Exception:
            continue
        if s.empty: 
            continue
        m = s[s.str.fullmatch(val_re, na=False)]
        for v in m.unique():
            v = v.strip()
            if v and v not in vals:
                vals.append(v)
                if len(vals) >= k:
                    return vals
    return vals

# =========================
# Agent
# =========================
class DBAgent:
    def __init__(self):
        # Catalog
        if not os.path.isfile(CAT):
            raise FileNotFoundError(f"catalog.json not found at {CAT}")
        with open(CAT, "r", encoding="utf-8") as f:
            self.cat = json.load(f)

        # Model (ALWAYS from models/nl2sql_tiny)
        self.model_dir = _latest_checkpoint(MODEL_BASE)
        spm_path = _find_sp_model(self.model_dir)
        print(f"[db-agent] model_dir: {self.model_dir}")
        print(f"[db-agent] spm:       {spm_path}")

        self.tok = T5Tokenizer(vocab_file=str(spm_path), legacy=True)
        if self.tok.pad_token is None: self.tok.add_special_tokens({"pad_token": "<pad>"})
        if self.tok.eos_token is None: self.tok.add_special_tokens({"eos_token": "</s>"})
        if self.tok.unk_token is None: self.tok.add_special_tokens({"unk_token": "<unk>"})
        if self.tok.bos_token is None: self.tok.add_special_tokens({"bos_token": "<s>"})
        self.model = T5ForConditionalGeneration.from_pretrained(self.model_dir)

        # DuckDB
        self.con = duckdb.connect(database=":memory:")
        self.con.execute("PRAGMA threads=4;")

    # ---------- NLâ†’SQL ----------
    def _gen_sql(self, question: str) -> str:
        ids = self.tok(question, return_tensors="pt", truncation=True, max_length=128)
        out = self.model.generate(**ids, max_length=128, num_beams=4, do_sample=False)
        sql = self.tok.decode(out[0], skip_special_tokens=True).strip().rstrip(";")
        if DEBUG: print(f"[db-agent] gen SQL: {sql}")
        return sql

    def _map_tables_to_parquet(self, sql: str) -> str:
        for key, meta in self.cat["tables"].items():
            path = _sql_escape_like(_norm_path(meta["parquet"]))
            pattern = rf'(?i)(?:["`])?{re.escape(key)}(?:["`])?'
            sql = re.sub(pattern, f"parquet('{path}')", sql)
        return sql

    # ---------- helpers ----------
    def _tables(self):
        return sorted([f"{v['schema']}.{v['table']}" for v in self.cat["tables"].values()], key=str.lower)

    def _best_table(self, frag: str):
        frag = frag.lower()
        best, best_score = None, -1
        for key, meta in self.cat["tables"].items():
            cand2 = meta["table"].lower()
            score = max(int(frag in key.lower())*3, int(frag in cand2)*3, len(os.path.commonprefix([frag, cand2])))
            if score > best_score:
                best_score = score
                best = (key, meta)
        return best

    # ---------- main ----------
    def ask(self, q: str) -> str:
        if not is_probably_db_query(q):
            return small_talk(q)

        L = q.lower().strip()

        # list tables
        if (("list" in L or "show" in L or "name" in L) and "table" in L) or L in {"tables","list tables","show tables"}:
            return "here are your tables:\n- " + "\n- ".join(self._tables())

        # top N table (data dump convenience)
        m = re.search(r"top\s+(\d+)\s+([A-Za-z0-9_.-]+)", L)
        if m:
            n = int(m.group(1)); frag = m.group(2)
            best = self._best_table(frag)
            if not best: return "i couldnâ€™t find a table matching that."
            _, meta = best
            p = _norm_path(meta["parquet"])
            try:
                df = self.con.execute(f"SELECT * FROM parquet('{_sql_escape_like(p)}') LIMIT {n}").fetchdf()
                return "here you go:\n" + (df.to_string(index=False) if not df.empty else "(no rows)")
            except Exception:
                return "sorry, i couldnâ€™t read that table."

        # direct phone/email ask â†’ do row-only search first
        want = _wanted_field(q)
        if want in {"phone","email"}:
            hint = _extract_name_hint(q)
            if hint:
                txt = self._phone_email_lookup_rows_only(hint, want)
                if txt:
                    return txt

        # NLâ†’SQL path
        sql = self._gen_sql(q)
        if re.match(r"^\s*select\b", sql, flags=re.IGNORECASE):
            try:
                sql2 = self._map_tables_to_parquet(sql)
                if DEBUG: print(f"[db-agent] remapped SQL: {sql2}")
                df = self.con.execute(sql2).fetchdf()
                if not df.empty:
                    return self._format_answer(q, df)
            except Exception as e:
                if DEBUG: print(f"[db-agent] SQL error: {e}")

        # fallback: row-only lexical scan
        return self._fallback_row_only(q)

    # ---------- Row-only phone/email ----------
    def _phone_email_lookup_rows_only(self, name_hint: str, want: str) -> str | None:
        tokens = [name_hint] + name_hint.split()
        tokens = list(dict.fromkeys([t for t in tokens if t]))
        val_re   = PHONE_VAL_RE if want == "phone" else EMAIL_VAL_RE
        want_lbl = "phone" if want == "phone" else "email"
        results = []

        for idx, (key, meta) in enumerate(self.cat["tables"].items()):
            if idx >= TABLE_SCAN_LIMIT:
                break
            p = _norm_path(meta["parquet"]); p_esc = _sql_escape_like(p)
            # sample to learn columns only
            try:
                probe = self.con.execute(
                    f"SELECT * FROM parquet('{p_esc}') LIMIT {min(ROW_LIMIT_PER_TABLE, 50)}"
                ).fetchdf()
            except Exception:
                continue
            if probe.empty: 
                continue
            cols = list(probe.columns)
            rowtxt = _row_text_expr(cols)
            per_tok = []
            for t in tokens:
                t_esc = _sql_escape_like(t)
                per_tok.append(f"({rowtxt}) ILIKE '%{t_esc}%'")
            where = " AND ".join(per_tok) if len(per_tok) > 1 else per_tok[0]
            q2 = f"SELECT * FROM parquet('{p_esc}') WHERE {where} LIMIT 200"
            try:
                df = self.con.execute(q2).fetchdf()
            except Exception:
                continue
            if df.empty: 
                continue

            picks = _extract_values_by_regex(df, val_re, k=5 - len(results))
            for v in picks:
                if v not in results:
                    results.append(v)
                    if len(results) >= 5:
                        nm = name_hint.strip() or "the match"
                        return f"{want_lbl} for {nm}: " + ", ".join(results)

        if results:
            nm = name_hint.strip() or "the match"
            return f"{want_lbl} for {nm}: " + ", ".join(results)
        return None

    # ---------- Row-only lexical fallback ----------
    def _fallback_row_only(self, q: str) -> str:
        toks = _tokens_from_question(q)
        if not toks:
            return "hmm, i didnâ€™t get a match. can you add more detail, maybe the full name in quotes?"

        # Pass A: AND across tokens (row-only)
        hit = self._scan_tables_row_only(toks, mode="AND")
        if hit: return hit

        # Pass B: OR across tokens
        hit = self._scan_tables_row_only(toks, mode="OR")
        if hit: return hit

        # Pass C: Longest token only, OR
        hit = self._scan_tables_row_only([max(toks, key=len)], mode="OR")
        if hit: return hit

        return "i couldnâ€™t find that. try adding a bit more detail, e.g. full name in quotes."

    def _scan_tables_row_only(self, tokens, mode="AND"):
        count = 0
        for key, meta in self.cat["tables"].items():
            if count >= TABLE_SCAN_LIMIT:
                break
            p = _norm_path(meta["parquet"]); p_esc = _sql_escape_like(p)
            # Peek columns to build row text expr (we do not match on column names)
            try:
                probe = self.con.execute(
                    f"SELECT * FROM parquet('{p_esc}') LIMIT {min(ROW_LIMIT_PER_TABLE, 50)}"
                ).fetchdf()
            except Exception:
                continue
            if probe.empty:
                continue
            count += 1

            cols = list(probe.columns)
            rowtxt = _row_text_expr(cols)

            per_tok = []
            for t in tokens:
                t_esc = _sql_escape_like(t)
                per_tok.append(f"({rowtxt}) ILIKE '%{t_esc}%'")

            where = (" AND ".join(per_tok)) if mode == "AND" else (" OR ".join(per_tok))
            q2 = f"SELECT * FROM parquet('{p_esc}') WHERE {where} LIMIT 5"

            try:
                df = self.con.execute(q2).fetchdf()
            except Exception:
                continue

            if not df.empty:
                return f"i found matches in {meta['schema']}.{meta['table']}:\n" + df.to_string(index=False)

        return None

    # ---------- format answers ----------
    def _format_answer(self, q: str, df: pd.DataFrame) -> str:
        want = _wanted_field(q)
        if df.shape == (1,1):
            val = str(df.iat[0,0]).strip()
            if want == "phone": return f"phone: {val}" if val else "i didnâ€™t find a phone there."
            if want == "email": return f"email: {val}" if val else "i didnâ€™t find an email there."
            return val or "no value found."

        # for phone/email, extract purely by regex from data (no column-name bias)
        if want == "phone":
            vals = _extract_values_by_regex(df, PHONE_VAL_RE, k=5)
            if vals:
                name_hint = _extract_name_hint(q)
                return f"phone for {name_hint or 'the match'}: " + ", ".join(vals)
        if want == "email":
            vals = _extract_values_by_regex(df, EMAIL_VAL_RE, k=5)
            if vals:
                name_hint = _extract_name_hint(q)
                return f"email for {name_hint or 'the match'}: " + ", ".join(vals)

        # fallback: compact table
        return "hereâ€™s what i found:\n" + df.head(5).to_string(index=False)

# -------------- CLI --------------
if __name__ == "__main__":
    if not os.path.isfile(CAT):
        print(f"catalog.json not found at {CAT}.")
        raise SystemExit(1)

    try:
        bot = DBAgent()
    except Exception as e:
        print(f"startup error: {e}")
        print("\nChecklist:\n"
              "1) Ensure model weights exist under models/nl2sql_tiny or its checkpoint-* subfolder "
              "(pytorch_model.bin or model.safetensors).\n"
              "2) Ensure SentencePiece file exists there (spiece.model/spm.model/sentencepiece.model).\n")
        raise

    print("chat + db agent (nl2sql_tiny + row-only fallback) ready. try:\n"
          "- phone number of \"rajendra shikwal\"\n"
          "- email of PZ6115\n"
          "- name of all tables\n"
          "- hi / help / tell me a joke")
    while True:
        try:
            s = input("\n> ").strip()
        except (EOFError, KeyboardInterrupt):
            break
        if not s or s.lower() in {"exit","quit"}:
            break
        print(bot.ask(s))
