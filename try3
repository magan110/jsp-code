using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Mvc;
using PdfiumViewer; // Keep this for PDF->Image conversion
using System;
using System.Collections.Generic;
using System.Drawing.Imaging;
using System.IO;
using System.Linq;
using System.Net.Http;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

namespace ocr.Controllers
{
    [Route("api/[controller]")]
    [ApiController]
    public class OCRController : ControllerBase
    {
        // REPLACE THIS WITH YOUR ACTUAL GOOGLE API KEY
        private const string GoogleApiKey = "AIzaSyDJmNF3BQQ4HWtzuX80j6IvCshwQZPBgjk";

        // Gemini 1.5 Flash or 2.0 Flash are best for speed/cost with Vision
        private const string GeminiEndpoint = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent";

        private static readonly HttpClient _httpClient = new HttpClient();

        public OCRController()
        {
        }

        [HttpPost("upload")]
        public async Task<IActionResult> UploadAndExtract(IFormFile file)
        {
            if (file == null || file.Length == 0) return BadRequest("Invalid file.");

            try
            {
                List<byte[]> imagesToSend = new List<byte[]>();
                string mimeType = "image/jpeg"; // Default extraction format

                var extension = Path.GetExtension(file.FileName).ToLower();

                // 1. PREPARE IMAGES (Handle PDF vs Image)
                using (var memoryStream = new MemoryStream())
                {
                    await file.CopyToAsync(memoryStream);
                    byte[] fileBytes = memoryStream.ToArray();

                    if (extension == ".pdf")
                    {
                        // Convert PDF pages to Images so Gemini can "see" them
                        // Note: Gemini API accepts PDFs directly in some workflows, 
                        // but converting to Image ensures consistency across all file types.
                        using (var pdfDocument = PdfDocument.Load(new MemoryStream(fileBytes)))
                        {
                            // Only processing the first page for the bill to save tokens/complexity.
                            // If bills are multi-page, loop this.
                            int pageIndex = 0;
                            var size = pdfDocument.PageSizes[pageIndex];
                            int width = (int)(size.Width / 72.0 * 300); // 300 DPI
                            int height = (int)(size.Height / 72.0 * 300);

                            using (var bitmap = pdfDocument.Render(pageIndex, width, height, 300, 300, true))
                            using (var imageStream = new MemoryStream())
                            {
                                bitmap.Save(imageStream, ImageFormat.Jpeg);
                                imagesToSend.Add(imageStream.ToArray());
                            }
                        }
                    }
                    else
                    {
                        // It's already an image (jpg, png, etc)
                        imagesToSend.Add(fileBytes);
                        if (extension == ".png") mimeType = "image/png";
                    }
                }

                // 2. SEND IMAGE DIRECTLY TO GEMINI (Vision API)
                // We skip Tesseract entirely. Gemini reads the handwriting.
                Dictionary<string, object> kvPairs;
                try
                {
                    // We only send the first image found.
                    kvPairs = await AnalyzeImageWithGemini(imagesToSend.First(), mimeType);
                }
                catch (Exception ex)
                {
                    return StatusCode(500, $"AI Processing Error: {ex.Message}");
                }

                return Ok(new
                {
                    FileName = file.FileName,
                    ExtractionMethod = "Gemini Vision (Handwriting Supported)",
                    Data = kvPairs
                });
            }
            catch (Exception ex)
            {
                return StatusCode(500, $"Internal Server Error: {ex.Message}");
            }
        }

        private async Task<Dictionary<string, object>> AnalyzeImageWithGemini(byte[] imageBytes, string mimeType)
        {
            string base64Image = Convert.ToBase64String(imageBytes);

            // 1. The Prompt
            // We specifically ask it to handle handwriting.
            string promptText = @"
    You are an expert AI specialized in digitizing handwritten documents.
    
    TASK: 
    Analyze the image and extract EVERY meaningful field you see as a Key-Value pair. 
    There is no fixed schema. You must discover the fields dynamically based on the document content.

    RULES FOR KEYS (The Labels):
    1. Look for labels on the paper (e.g., 'Date:', 'Guest Name', 'Bill No.', 'G. Total').
    2. Convert these labels into clean snake_case JSON keys (e.g., 'Bill No.' -> ""bill_no"", 'Room #' -> ""room_number"").
    3. If a field has no clear label but is obvious (like the Hotel Name at the top), create a logical key like ""merchant_name"".

    RULES FOR VALUES (The Content):
    1. Transcribe handwriting with extreme accuracy.
    2. Maintain original formatting for values (do not strip currency symbols or change date formats).
    3. If a value is crossed out, ignore it. Read the final valid value.
    4. Capture ALL specific details found (e.g., Check-in Time, GSTIN, Pax, Advance, Remarks).

    RETURN RAW JSON ONLY.";

            // 2. Construct Payload for Multimodal Request
            var requestBody = new
            {
                contents = new[]
                {
                    new
                    {
                        parts = new object[]
                        {
                            new { text = promptText },
                            new
                            {
                                inline_data = new
                                {
                                    mime_type = mimeType,
                                    data = base64Image
                                }
                            }
                        }
                    }
                }
            };

            var jsonContent = new StringContent(
                JsonSerializer.Serialize(requestBody),
                Encoding.UTF8,
                "application/json");

            // 3. Call API
            var response = await _httpClient.PostAsync($"{GeminiEndpoint}?key={GoogleApiKey}", jsonContent);

            var responseString = await response.Content.ReadAsStringAsync();

            if (!response.IsSuccessStatusCode)
            {
                throw new Exception($"Google API Error: {response.StatusCode} - {responseString}");
            }

            // 4. Parse Response
            using (JsonDocument doc = JsonDocument.Parse(responseString))
            {
                try
                {
                    var root = doc.RootElement;
                    var textPart = root
                        .GetProperty("candidates")[0]
                        .GetProperty("content")
                        .GetProperty("parts")[0]
                        .GetProperty("text")
                        .GetString();

                    // Clean Markdown
                    textPart = textPart.Replace("```json", "").Replace("```", "").Trim();

                    return JsonSerializer.Deserialize<Dictionary<string, object>>(textPart);
                }
                catch
                {
                    return new Dictionary<string, object> { { "RawResponse", responseString }, { "Error", "Failed to parse JSON" } };
                }
            }
        }
    }
}
