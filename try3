import os, json, re, glob, random, datetime, difflib, unicodedata
import duckdb
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration

# ----------------- FIXED MODEL LOCATION -----------------
MODEL_BASE  = "models/nl2sql_tiny"     # << always use this
# --------------------------------------------------------

DB_DIR      = "../db_export_bwlive"
CAT         = os.path.join(DB_DIR, "catalog.json")
DEBUG       = os.environ.get("AI_DB_AGENT_DEBUG", "0") == "1"
TABLE_SCAN_LIMIT = int(os.environ.get("AI_DB_AGENT_TABLE_LIMIT", "300"))

# small-talk heuristics
GREET_WORDS = {"hi","hello","hey","yo","namaste","hola","sup","hii","hiii","hlo"}
THANK_WORDS = {"thanks","thank you","ty","thx","shukriya","dhanyavaad"}
BYE_WORDS   = {"bye","goodbye","see ya","see you","tata"}
HELP_WORDS  = {"help","commands","examples","how to","what can you do"}
JOKE_WORDS  = {"joke","funny","make me laugh"}

STOPWORDS = {
    "what","is","are","of","for","about","the","a","an","this","that","these","those",
    "show","give","get","find","list","name","names","code","codes","number","numbers",
    "top","first","last","all","and","or","with","without","please","pls","zone",
}

PHONE_COL_RE = re.compile(r"(mob|phone|phon|tel|contact|cell)", re.I)
EMAIL_COL_RE = re.compile(r"(email|e\-?mail|mailid|mail_id|mail)", re.I)
PHONE_VAL_RE = re.compile(r"^\s*\+?\s*(?:\d[\s().-]?){7,15}\s*$")
EMAIL_VAL_RE = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")

def _norm_path(p: str) -> str:
    return p.replace("\\", "/")

def _sql_escape_like(s: str) -> str:
    return (s or "").replace("'", "''")

def _clean_str(x) -> str:
    s = str(x or "").strip()
    return unicodedata.normalize("NFKC", s)

def _wanted_field(q: str):
    L = q.lower()
    if re.search(r"\b(email|e\-?mail)\b", L): return "email"
    if re.search(r"\b(mobile|phone|contact|tel|cell)\b", L): return "phone"
    return None

def _tokens_from_question(q: str):
    m = re.search(r"[\"‚Äú‚Äù'`](.+?)[\"‚Äú‚Äù'`]", q)
    raw = re.findall(r"[A-Za-z0-9@+._/-]+", m.group(1) if m else q)
    toks = []
    for t in raw:
        tl = t.lower()
        if tl in STOPWORDS: continue
        if any(c.isdigit() for c in t) or len(t) >= 3:
            toks.append(t)
    if not toks and raw:
        toks = [max(raw, key=len)]
    return toks[:6]

def _extract_name_hint(q: str):
    m = re.search(r"[\"‚Äú‚Äù'`](.+?)[\"‚Äú‚Äù'`]", q)
    if m: return m.group(1).strip()
    t = _tokens_from_question(q)
    return " ".join(t[:3]) if t else ""

def _looks_like_phone_series(s: pd.Series) -> float:
    if s is None or s.empty: return 0.0
    try:
        vals = s.dropna().astype(str)
        return float(vals.str.fullmatch(PHONE_VAL_RE, na=False).sum()) / max(1, len(vals))
    except Exception:
        return 0.0

def _looks_like_email_series(s: pd.Series) -> float:
    if s is None or s.empty: return 0.0
    try:
        vals = s.dropna().astype(str)
        return float(vals.str.fullmatch(EMAIL_VAL_RE, na=False).sum()) / max(1, len(vals))
    except Exception:
        return 0.0

def _has_model_files(d: str) -> bool:
    pats = [
        "pytorch_model.bin", "model.safetensors",
        "pytorch_model-*.bin", "model-*.safetensors",
        "pytorch_model.bin.index.json", "model.safetensors.index.json",
    ]
    return any(glob.glob(os.path.join(d, p)) for p in pats)

def _latest_checkpoint(base: str) -> str:
    if not os.path.isdir(base):
        raise FileNotFoundError(f"Model folder not found: {base}")
    cands = []
    for name in os.listdir(base):
        full = os.path.join(base, name)
        if os.path.isdir(full) and name.startswith("checkpoint-") and _has_model_files(full):
            try: cands.append((int(name.split("-")[-1]), full))
            except: pass
    if cands:
        cands.sort(key=lambda x: x[0])
        return cands[-1][1]
    if _has_model_files(base): return base
    raise FileNotFoundError(
        f"No usable model files found in {base}. Put weights in {base} or in checkpoint-* subfolders "
        f"(expected pytorch_model.bin or model.safetensors)."
    )

def _find_sp_model(model_dir: str) -> str:
    for name in ("spiece.model","spm.model","sentencepiece.model","tokenizer.model"):
        p = os.path.join(model_dir, name)
        if os.path.isfile(p): return os.path.abspath(p)
    # also check the model base if we selected a checkpoint subdir
    base = os.path.dirname(model_dir)
    for name in ("spiece.model","spm.model","sentencepiece.model","tokenizer.model"):
        p = os.path.join(base, name)
        if os.path.isfile(p): return os.path.abspath(p)
    raise FileNotFoundError(
        f"SentencePiece model not found in {model_dir} or its base.\n"
        f"Expected one of: spiece.model / spm.model / sentencepiece.model / tokenizer.model"
    )

def is_probably_db_query(q: str) -> bool:
    L = q.lower()
    cues = [
        r"\b(phone|mobile|contact|tel|email|e\-?mail)\b",
        r"\b(code|customer|cust|retail|retl|party|employee|emp|sap|invoice|docu|pin|pincode)\b",
        r"\b(list|show|top|count|where|select|table|rows?)\b",
        r"[A-Za-z]{2,}\d{2,}",  # IDs like PZ6115, 4101P158
    ]
    return any(re.search(x, L) for x in cues)

def small_talk(q: str) -> str:
    L = q.strip().lower()
    words = set(re.findall(r"[a-z]+", L))
    if words & GREET_WORDS:
        return random.choice(["hey! how can i help?","hello! ask me anything about your data‚Äîor just chat üôÇ","hi! need a phone/email/code or some quick info?"])
    if words & THANK_WORDS:
        return random.choice(["anytime!","you‚Äôre welcome ‚ú®","glad to help."])
    if words & BYE_WORDS:
        return random.choice(["bye! have a good one üëã","see you later!","catch you soon!"])
    if any(x in L for x in HELP_WORDS):
        return ("i‚Äôm your data assistant + chat buddy. try:\n"
                "‚Ä¢ phone number of \"rajendra shikwal\"\n"
                "‚Ä¢ email of PZ6115\n"
                "‚Ä¢ top 5 adtAnnGifts\n"
                "‚Ä¢ name of all tables\n"
                "you can also just chat: ‚Äútell me a joke‚Äù, ‚Äúwhat can you do‚Äù, etc.")
    if "time" in L or "date" in L or "today" in L:
        now = datetime.datetime.now()
        return f"it‚Äôs {now.strftime('%Y-%m-%d %H:%M:%S')} on your machine."
    if any(x in L for x in JOKE_WORDS):
        return random.choice([
            "why did the database admin break up with sql? too many joins. üòÖ",
            "i told my boss i know sql. they said: prove it. i replied: SELECT * FROM compliments;",
        ])
    return random.choice(["got it‚Äîask me anything about your database, or say 'help' for examples.",
                          "i‚Äôm here. what do you want to find?",
                          "cool. tell me who/what you‚Äôre looking for and i‚Äôll fetch it."])

class DBAgent:
    def __init__(self):
        # Catalog
        if not os.path.isfile(CAT):
            raise FileNotFoundError(f"catalog.json not found at {CAT}")
        with open(CAT, "r", encoding="utf-8") as f:
            self.cat = json.load(f)

        # Model (ALWAYS from models/nl2sql_tiny)
        self.model_dir = _latest_checkpoint(MODEL_BASE)
        spm_path = _find_sp_model(self.model_dir)
        print(f"[db-agent] model_dir: {self.model_dir}")
        print(f"[db-agent] spm:       {spm_path}")

        self.tok = T5Tokenizer(vocab_file=str(spm_path), legacy=True)
        if self.tok.pad_token is None: self.tok.add_special_tokens({"pad_token": "<pad>"})
        if self.tok.eos_token is None: self.tok.add_special_tokens({"eos_token": "</s>"})
        if self.tok.unk_token is None: self.tok.add_special_tokens({"unk_token": "<unk>"})
        if self.tok.bos_token is None: self.tok.add_special_tokens({"bos_token": "<s>"})

        self.model = T5ForConditionalGeneration.from_pretrained(self.model_dir)

        # DuckDB
        self.con = duckdb.connect(database=":memory:")
        self.con.execute("PRAGMA threads=4;")

    # ---------- NL‚ÜíSQL ----------
    def _gen_sql(self, question: str) -> str:
        ids = self.tok(question, return_tensors="pt", truncation=True, max_length=128)
        out = self.model.generate(**ids, max_length=128, num_beams=4, do_sample=False)
        sql = self.tok.decode(out[0], skip_special_tokens=True).strip().rstrip(";")
        if DEBUG: print(f"[db-agent] gen SQL: {sql}")
        return sql

    def _map_tables_to_parquet(self, sql: str) -> str:
        for key, meta in self.cat["tables"].items():
            path = _sql_escape_like(_norm_path(meta["parquet"]))
            pattern = rf'(?i)(?:["`])?{re.escape(key)}(?:["`])?'
            sql = re.sub(pattern, f"parquet('{path}')", sql)
        return sql

    # ---------- helpers ----------
    def _tables(self):
        return sorted([f"{v['schema']}.{v['table']}" for v in self.cat["tables"].values()], key=str.lower)

    def _best_table(self, frag: str):
        frag = frag.lower()
        best, best_score = None, -1
        for key, meta in self.cat["tables"].items():
            cand2 = meta["table"].lower()
            score = max(int(frag in key.lower())*3, int(frag in cand2)*3, len(os.path.commonprefix([frag, cand2])))
            if score > best_score:
                best_score = score
                best = (key, meta)
        return best

    # ---------- main ----------
    def ask(self, q: str) -> str:
        if not is_probably_db_query(q):
            return small_talk(q)

        L = q.lower().strip()

        # list tables
        if (("list" in L or "show" in L or "name" in L) and "table" in L) or L in {"tables","list tables","show tables"}:
            return "here are your tables:\n- " + "\n- ".join(self._tables())

        # top N table
        m = re.search(r"top\s+(\d+)\s+([A-Za-z0-9_.-]+)", L)
        if m:
            n = int(m.group(1)); frag = m.group(2)
            best = self._best_table(frag)
            if not best: return "i couldn‚Äôt find a table matching that."
            _, meta = best
            p = _norm_path(meta["parquet"])
            try:
                df = self.con.execute(f"SELECT * FROM parquet('{_sql_escape_like(p)}') LIMIT {n}").fetchdf()
                return "here you go:\n" + (df.to_string(index=False) if not df.empty else "(no rows)")
            except Exception:
                return "sorry, i couldn‚Äôt read that table."

        # targeted quick asks (phone/email)
        want = _wanted_field(q)
        if want in {"phone","email"}:
            hint = _extract_name_hint(q)
            if hint:
                # try lexical fetch first across columns with name-ish values
                txt = self._phone_email_lookup(hint, want)
                if txt: return txt

        # NL‚ÜíSQL path
        sql = self._gen_sql(q)
        if re.match(r"^\s*select\b", sql, flags=re.IGNORECASE):
            try:
                sql2 = self._map_tables_to_parquet(sql)
                if DEBUG: print(f"[db-agent] remapped SQL: {sql2}")
                df = self.con.execute(sql2).fetchdf()
                if not df.empty:
                    return self._format_answer(q, df)
            except Exception as e:
                if DEBUG: print(f"[db-agent] SQL error: {e}")

        # fallback lexical scan
        return self._fallback_scan(q)

    # ---------- phone/email helper ----------
    def _phone_email_lookup(self, name_hint: str, want: str) -> str|None:
        # scan a subset of tables quickly
        tokens = [name_hint] + name_hint.split()
        tokens = list(dict.fromkeys([t for t in tokens if t]))
        for key, meta in list(self.cat["tables"].items())[:TABLE_SCAN_LIMIT]:
            p = _norm_path(meta["parquet"])
            try:
                probe = self.con.execute(f"SELECT * FROM parquet('{_sql_escape_like(p)}') LIMIT 300").fetchdf()
            except Exception:
                continue
            if probe.empty: continue

            # build OR where on likely "name-ish" columns (or all text columns)
            cols = list(probe.columns)
            nameish = [c for c in cols if any(k in c.lower() for k in ["name","party","person","cust","retl","owner","emp","contact"])]
            target_cols = nameish or cols

            per_tok = []
            for t in tokens:
                t_esc = _sql_escape_like(t)
                ors = [f"CAST(\"{c}\" AS VARCHAR) ILIKE '%{t_esc}%'" for c in target_cols]
                per_tok.append("(" + " OR ".join(ors) + ")")

            where = " AND ".join(per_tok) if len(per_tok) > 1 else per_tok[0]
            q2 = f"SELECT * FROM parquet('{_sql_escape_like(p)}') WHERE {where} LIMIT 50"
            try:
                df = self.con.execute(q2).fetchdf()
            except Exception:
                continue
            if df.empty: continue

            picks = self._extract_wanted_values(df, want, k=5)
            if picks:
                pretty = ", ".join(picks)
                return f"{want} for {name_hint}: {pretty}"
        return None

    # ---------- lexical scan fallback ----------
    def _fallback_scan(self, q: str) -> str:
        toks = _tokens_from_question(q)
        if not toks:
            return "hmm, i didn‚Äôt get a match. can you add more detail, maybe the full name in quotes?"

        for mode in ("AND","OR"):
            hit = self._scan_tables(toks, mode=mode)
            if hit: return hit

        hit = self._scan_tables([max(toks, key=len)], mode="OR")
        if hit: return hit

        return "i couldn‚Äôt find that. try adding a bit more detail, e.g. full name in quotes."

    def _scan_tables(self, tokens, mode="AND"):
        count = 0
        for key, meta in self.cat["tables"].items():
            if count >= TABLE_SCAN_LIMIT: break
            p = _norm_path(meta["parquet"])
            try:
                probe = self.con.execute(f"SELECT * FROM parquet('{_sql_escape_like(p)}') LIMIT 200").fetchdf()
            except Exception:
                continue
            if probe.empty: continue
            count += 1

            cols = list(probe.columns)
            per_tok = []
            for t in tokens:
                t_esc = _sql_escape_like(t)
                ors = [f"CAST(\"{c}\" AS VARCHAR) ILIKE '%{t_esc}%'" for c in cols]
                per_tok.append("(" + " OR ".join(ors) + ")")
            where = (" AND ".join(per_tok)) if mode == "AND" else (" OR ".join(per_tok))
            q2 = f"SELECT * FROM parquet('{_sql_escape_like(p)}') WHERE {where} LIMIT 5"
            try:
                df = self.con.execute(q2).fetchdf()
            except Exception:
                continue
            if not df.empty:
                return f"i found matches in {meta['schema']}.{meta['table']}:\n" + df.to_string(index=False)
        return None

    # ---------- format answers ----------
    def _format_answer(self, q: str, df: pd.DataFrame) -> str:
        want = _wanted_field(q)
        if df.shape == (1,1):
            val = str(df.iat[0,0]).strip()
            if want == "phone": return f"phone: {val}" if val else "i didn‚Äôt find a phone there."
            if want == "email": return f"email: {val}" if val else "i didn‚Äôt find an email there."
            return val or "no value found."

        if want in {"phone","email"}:
            vals = self._extract_wanted_values(df, want, k=5)
            if vals:
                name_hint = _extract_name_hint(q)
                pretty = ", ".join(vals)
                return f"{want} for {name_hint or 'the match'}: {pretty}"

        return "here‚Äôs what i found:\n" + df.head(5).to_string(index=False)

    def _extract_wanted_values(self, df: pd.DataFrame, want: str, k=5):
        scores = []
        sample = df.head(200)
        for c in df.columns:
            s = sample[c]
            name_bonus = 0.0
            pat_frac = 0.0
            try:
                if want == "phone":
                    if PHONE_COL_RE.search(c): name_bonus += 0.6
                    pat_frac = _looks_like_phone_series(s)
                elif want == "email":
                    if EMAIL_COL_RE.search(c): name_bonus += 0.6
                    pat_frac = _looks_like_email_series(s)
            except Exception:
                pass
            score = name_bonus + pat_frac
            if score > 0.15:
                scores.append((c, score))
        scores.sort(key=lambda x: x[1], reverse=True)
        vals = []
        val_re = PHONE_VAL_RE if want == "phone" else EMAIL_VAL_RE
        for c,_ in scores[:6]:
            series = df[c].dropna().astype(str)
            series = series[series.str.fullmatch(val_re, na=False)]
            for v in series.unique():
                v = v.strip()
                if v and v not in vals:
                    vals.append(v)
                    if len(vals) >= k:
                        return vals
        return vals

# -------------- CLI --------------
if __name__ == "__main__":
    if not os.path.isfile(CAT):
        print(f"catalog.json not found at {CAT}.")
        raise SystemExit(1)

    try:
        bot = DBAgent()
    except Exception as e:
        print(f"startup error: {e}")
        print("\nChecklist:\n"
              "1) Ensure model weights exist under models/nl2sql_tiny or its checkpoint-* subfolder "
              "(pytorch_model.bin or model.safetensors).\n"
              "2) Ensure SentencePiece file exists there (spiece.model/spm.model/sentencepiece.model).\n")
        raise

    print("chat + db agent (nl2sql_tiny) ready. try:\n"
          "- phone number of \"rajendra shikwal\"\n"
          "- email of PZ6115\n"
          "- name of all tables\n"
          "- hi / help / tell me a joke")
    while True:
        try:
            s = input("\n> ").strip()
        except (EOFError, KeyboardInterrupt):
            break
        if not s or s.lower() in {"exit","quit"}:
            break
        print(bot.ask(s))
